---
type: principle
source:
  - "[[governed_memory_pitch|Governed Memory for the AI Era]]"
  - "[[ai-memory-validator-onepager|AI Memory Validator]]"
status: seed
---
# Anti-automation, pro-sovereignty
*○ principle*

AI should propose, humans should decide. Fully automated AI summarization is untrustworthy — it hallucsinates meaning, summarizes incorrectly, stores too much noise, and loses user trust over time. The system must be human-in-the-loop throughout, with plain text and portability ensuring vendor independence.

## Why it matters

This is the philosophical stance that distinguishes the vision from competitors who automate everything. Trust requires human governance. Sovereignty requires plain text and portability. "Humans decide" includes both deliberate acts (swipe to confirm) and behavioral patterns (choosing to use a piece of knowledge, not correcting it). Both are human agency — the definition of "decide" expands beyond conscious review. See [[implicit-validation-signals|Implicit Validation Signals]].

## Connections

This stance is implemented by [[human-in-the-loop-promotion|Human-in-the-loop promotion]], [[proactive-memory-review|Proactive Memory Review]], and [[implicit-validation-signals|Implicit Validation Signals]], which ensure that knowledge enters the trusted layer through human signals — explicit or implicit. The result is the [[trusted-knowledge-layer|Trusted Knowledge Layer]] — a corpus users can rely on precisely because they governed its creation. This principle contrasts directly with the automatic AI memory approach described in [[ai-memory-is-unreviewed|AI memory is unreviewed]], where convenience is traded for trust.
